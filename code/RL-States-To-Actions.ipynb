{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL-Playground-Clean.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziKeQ_SwOpWa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6c79f14-118f-4a10-8afc-9836a1802228"
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "\n",
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: setuptools in /usr/local/lib/python3.7/dist-packages (54.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zwvoj__fOsJp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34466cb4-7b91-4c1e-8325-311fccc30d80"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from gym.wrappers import Monitor\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment and displaying it\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env\n",
        "\n",
        "  \n",
        "def query_environment(name):\n",
        "  env = gym.make(name)\n",
        "  spec = gym.spec(name)\n",
        "  print(f\"Action Space: {env.action_space}\")\n",
        "  print(f\"Observation Space: {env.observation_space}\")\n",
        "  print(f\"Max Episode Steps: {spec.max_episode_steps}\")\n",
        "  print(f\"Nondeterministic: {spec.nondeterministic}\")\n",
        "  print(f\"Reward Range: {env.reward_range}\")\n",
        "  print(f\"Reward Threshold: {spec.reward_threshold}\")\n",
        "\n",
        "query_environment(\"Pendulum-v0\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Action Space: Box(-2.0, 2.0, (1,), float32)\n",
            "Observation Space: Box(-8.0, 8.0, (3,), float32)\n",
            "Max Episode Steps: 200\n",
            "Nondeterministic: False\n",
            "Reward Range: (-inf, inf)\n",
            "Reward Threshold: None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5ZByZmMIjRK"
      },
      "source": [
        "class ReplayBuffer:\n",
        "\n",
        "  def __init__(self, mem_size, *input_dims, state_size, action_space_size):\n",
        "    self.mem_size = mem_size\n",
        "    self.mem_cntr = 0\n",
        "    self.state_size = state_size\n",
        "    self.action_space_size = action_space_size\n",
        "    self.state_memory = np.zeros((self.mem_size, self.state_size, 1), dtype=np.float32)\n",
        "    self.new_state_memory = np.zeros((self.mem_size, self.state_size, 1), dtype=np.float32)\n",
        "    self.action_memory = np.zeros((self.mem_size, self.action_space_size, ), dtype=np.int32)\n",
        "    self.reward_memory = np.zeros(self.mem_size, dtype=np.int32)\n",
        "    self.terminal_memory = np.zeros(self.mem_size, dtype=np.int32)\n",
        "    self.current_states = None\n",
        "\n",
        "  def store_transition(self, state, action, reward, state_, done):\n",
        "    index = self.mem_cntr % self.mem_size\n",
        "    self.state_memory[index] = state\n",
        "    self.new_state_memory[index] = state_\n",
        "    self.reward_memory[index] = reward\n",
        "    self.action_memory[index] = action\n",
        "    self.terminal_memory[index] = 1 - int(done)\n",
        "    self.mem_cntr += 1\n",
        "\n",
        "\n",
        "  def sample_buffer(self, batch_size):\n",
        "    max_mem = min(self.mem_cntr, self.mem_size)\n",
        "    batch = np.random.choice(max_mem, batch_size, replace=False)\n",
        "    states = self.state_memory[batch]\n",
        "    self.current_states = states\n",
        "    states_ = self.new_state_memory[batch]\n",
        "    rewards = self.reward_memory[batch]\n",
        "    actions = self.action_memory[batch]\n",
        "    terminal = self.terminal_memory[batch]\n",
        "    #actions = np.expand_dims(actions, axis=1)\n",
        "    return states, actions, rewards, states_, terminal\n",
        "\n",
        "  def sample_buffer_with_distance(self, batch_size, distance):\n",
        "    max_mem = min(self.mem_cntr, self.mem_size)\n",
        "    batch = np.random.choice(max_mem, batch_size, replace=False)\n",
        "    actions = []\n",
        "    for index, value in enumerate(batch):\n",
        "      should_pass = False\n",
        "      avg_action = [0.0 for _ in range(self.action_space_size)]\n",
        "      if abs(max_mem - 1 - value) <= distance:\n",
        "        np.delete(batch, index)\n",
        "        pass\n",
        "      for i in range(distance):\n",
        "        if self.terminal_memory[value + i] == True:\n",
        "          np.delete(batch, index)\n",
        "          should_pass = True\n",
        "          break\n",
        "      if should_pass == True:\n",
        "        pass\n",
        "      for i in range(distance):\n",
        "        avg_action = avg_action + self.action_memory[value + i]\n",
        "      avg_action = avg_action / distance\n",
        "      actions.append(avg_action)\n",
        "    states = self.state_memory[batch]\n",
        "    self.current_states = states\n",
        "    states_ = self.state_memory[batch + distance - 1]\n",
        "    rewards = self.reward_memory[batch]\n",
        "    terminal = self.terminal_memory[batch]\n",
        "    return states, actions, rewards, states_, terminal\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "381thIzvjZP7"
      },
      "source": [
        "class Agent:\n",
        "  def __init__(self, env, lr=0.01, state_size=4, action_size=2, optimal_state=[0.0, 0.0], gamma=0.01, epsilon=0.2, batch_size=5, num_epochs=1, epsilon_dec=1e-3, epsilon_end=0.01, mem_size=10000, layer_sizes=[64, 16], max_distance=5):\n",
        "    self.env = env\n",
        "    self.lr = lr\n",
        "    self.state_size = state_size\n",
        "    self.action_size = action_size\n",
        "    self.layer_sizes = layer_sizes\n",
        "    self.max_distance = max_distance\n",
        "    self.state_and_state_to_action_model = self.create_state_and_state_to_action_model(self.layer_sizes, obs_space_size=self.state_size)\n",
        "    #self.optimal_state = [0.0 for _ in range(self.state_size)] # cart position, angle\n",
        "    self.optimal_state = optimal_state\n",
        "    self.current_state = None\n",
        "    self.selected_action = None\n",
        "    self.prev_state = None\n",
        "    self.gamma = gamma\n",
        "    self.epsilon = epsilon\n",
        "    self.batch_size = batch_size\n",
        "    self.num_epochs = num_epochs\n",
        "    self.epsilon_dec = epsilon_dec\n",
        "    self.epsilon_end = epsilon_end\n",
        "    self.mem_size = mem_size\n",
        "    self.memory = ReplayBuffer(mem_size, (self.state_size, 1), state_size=self.state_size, action_space_size=self.action_size)\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "  def create_state_and_state_to_action_model(self, layer_sizes=[32, 16], obs_space_size=4):\n",
        "    current_state_input = Input(shape=(obs_space_size, ), name='currentStateInput')\n",
        "    target_state_input = Input(shape=(obs_space_size, ), name='targetStateInput')\n",
        "    x = tf.keras.layers.concatenate([current_state_input, target_state_input], axis=1)\n",
        "    hidden_1 = Dense(layer_sizes[0], activation='tanh')(x)\n",
        "    hidden_2 = Dense(layer_sizes[1], activation='tanh')(hidden_1)\n",
        "    action = Dense(self.action_size, activation='softmax', name='outputAction')(hidden_2)\n",
        "    model = tf.keras.Model(\n",
        "    inputs=[current_state_input, target_state_input],\n",
        "    outputs=action,\n",
        "    )\n",
        "    model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(self.lr),\n",
        "    loss=tf.keras.losses.CategoricalCrossentropy()\n",
        "    )\n",
        "    return model  \n",
        " \n",
        "\n",
        "\n",
        "  def update_network_batched(self, max_distance, randomize):\n",
        "    if self.memory.mem_cntr < self.batch_size:\n",
        "      return\n",
        "    distance = max_distance\n",
        "    if randomize is True:\n",
        "      distance = np.random.choice(range(2, max_distance+1))\n",
        "    states, actions, rewards, states_, done = self.memory.sample_buffer_with_distance(batch_size=self.batch_size, distance=distance)\n",
        "    self.state_and_state_to_action_model.fit(\n",
        "    {\"currentStateInput\": states, \"targetStateInput\": states_},\n",
        "    {\"outputAction\": np.expand_dims(np.array(actions), -1)},\n",
        "    epochs=self.num_epochs,\n",
        "    batch_size=self.batch_size,\n",
        "    verbose=0\n",
        "    )\n",
        "\n",
        "\n",
        "  def store_transition(self, state, action, reward, new_state, done):\n",
        "    self.memory.store_transition(np.expand_dims(state, axis=1), action, reward, np.expand_dims(new_state, axis=1) , done)\n",
        "\n",
        "\n",
        "  def choose_action_from_the_states(self, currentState, targetState):\n",
        "    if np.random.random() < self.epsilon:\n",
        "      action = np.random.choice(self.action_size)\n",
        "    else:\n",
        "      state = np.array([currentState])\n",
        "      predicted_action = self.state_and_state_to_action_model([{\"targetStateInput\": np.expand_dims(np.asarray(self.optimal_state, dtype=np.float32), axis=0), \"currentStateInput\": state}])\n",
        "      self.selected_action = predicted_action\n",
        "      action = np.argmax(predicted_action[0])\n",
        "    return action\n",
        "\n",
        "  def to_one_hot(self, action_index):\n",
        "    vec = np.zeros((self.action_size,))\n",
        "    vec[action_index] = 1\n",
        "    return vec  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kr-bE3hfniBA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7be6fed8-7ad6-49b0-df93-18377da4f929"
      },
      "source": [
        "#env = wrap_env(gym.make(\"CartPole-v1\"))\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "#env = gym.make(\"MountainCar-v0\")\n",
        "#To tune: num_games, batch_size, activation functions, distance, epsilon\n",
        "\n",
        "num_games = 100\n",
        "max_distance = 5\n",
        "batch_size = 16\n",
        "num_epochs = 1\n",
        "\n",
        "#CartPole\n",
        "state_size = 4\n",
        "action_size = 2\n",
        "optimal_state = [0.0, 0.0, 0.0, 0.0]\n",
        "\n",
        "#MountainCar\n",
        "# state_size = 2\n",
        "# action_size = 3\n",
        "# optimal_state = [-1.2, -0.07]\n",
        "\n",
        "agent = Agent(env, batch_size=batch_size, state_size = state_size, action_size=action_size, optimal_state=optimal_state,\n",
        "              num_epochs=num_epochs, lr=0.01, epsilon=0.05, epsilon_dec=0.01, epsilon_end=0.1, layer_sizes=[64, 64])\n",
        "rewards = [0.0 for _ in range(num_games)]\n",
        "steps = [0 for _ in range(num_games)]\n",
        "# state -> array of 4: 0 cart position, 1 cart velocity, 2 angle, 3 velocity at tip\n",
        "for index in range(num_games):\n",
        "  current_state = env.reset()\n",
        "  while True:\n",
        "      #env.render()\n",
        "      agent.prev_state = current_state\n",
        "      selected_action_index = agent.choose_action_from_the_states(current_state, agent.optimal_state)\n",
        "      agent.selected_action = agent.to_one_hot(selected_action_index)\n",
        "      new_state, reward, done, info = env.step(selected_action_index)\n",
        "      rewards[index] = rewards[index] + reward\n",
        "      agent.current_state = new_state\n",
        "      agent.store_transition( current_state, agent.selected_action, reward, new_state, done)\n",
        "      current_state = new_state\n",
        "      steps[index] += 1\n",
        "      if done: \n",
        "        break;\n",
        "  if agent.epsilon >= agent.epsilon_end + agent.epsilon_dec:\n",
        "    agent.epsilon = agent.epsilon - agent.epsilon_dec\n",
        "\n",
        "  agent.update_network_batched(max_distance, randomize=False)\n",
        "  \n",
        "\n",
        "#print(sum(steps) / num_games)\n",
        "print(sum(rewards) / num_games)  \n",
        "print(np.median(rewards))   \n",
        "print(rewards)      \n",
        "env.close()\n",
        "#show_video()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "142.53\n",
            "110.5\n",
            "[62.0, 35.0, 63.0, 104.0, 101.0, 104.0, 119.0, 66.0, 46.0, 89.0, 135.0, 142.0, 128.0, 92.0, 49.0, 83.0, 50.0, 113.0, 68.0, 78.0, 99.0, 129.0, 157.0, 253.0, 271.0, 313.0, 254.0, 318.0, 142.0, 138.0, 104.0, 121.0, 148.0, 176.0, 74.0, 96.0, 94.0, 107.0, 114.0, 106.0, 128.0, 148.0, 261.0, 110.0, 62.0, 50.0, 60.0, 67.0, 96.0, 65.0, 57.0, 43.0, 89.0, 126.0, 95.0, 52.0, 78.0, 55.0, 96.0, 95.0, 92.0, 113.0, 305.0, 209.0, 72.0, 52.0, 77.0, 100.0, 81.0, 88.0, 216.0, 500.0, 500.0, 500.0, 302.0, 393.0, 237.0, 151.0, 138.0, 212.0, 150.0, 161.0, 152.0, 307.0, 165.0, 320.0, 156.0, 158.0, 142.0, 144.0, 111.0, 85.0, 94.0, 92.0, 88.0, 129.0, 104.0, 229.0, 221.0, 133.0]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}