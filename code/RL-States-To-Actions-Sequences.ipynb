{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL-Playground-With-Sequences.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziKeQ_SwOpWa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "outputId": "6ec44466-45d8-40d2-8f29-21bf45c72d70"
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "\n",
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1\n",
        "!pip install scipy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting setuptools\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/16/e9f5c5b86696da09298ea10c32d68ad8ea21f888e45b11aa9e615adda6c9/setuptools-49.2.1-py3-none-any.whl (789kB)\n",
            "\u001b[K     |████████████████████████████████| 798kB 4.6MB/s \n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: setuptools\n",
            "  Found existing installation: setuptools 49.2.0\n",
            "    Uninstalling setuptools-49.2.0:\n",
            "      Successfully uninstalled setuptools-49.2.0\n",
            "Successfully installed setuptools-49.2.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pkg_resources"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scipy) (1.18.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zwvoj__fOsJp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "265a073f-7cb7-4565-afb8-b22249ae4962"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import scipy\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Input, BatchNormalization\n",
        "from gym.wrappers import Monitor\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment and displaying it\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env\n",
        "\n",
        "  \n",
        "def query_environment(name):\n",
        "  env = gym.make(name)\n",
        "  spec = gym.spec(name)\n",
        "  print(f\"Action Space: {env.action_space}\")\n",
        "  print(f\"Observation Space: {env.observation_space}\")\n",
        "  print(f\"Max Episode Steps: {spec.max_episode_steps}\")\n",
        "  print(f\"Nondeterministic: {spec.nondeterministic}\")\n",
        "  print(f\"Reward Range: {env.reward_range}\")\n",
        "  print(f\"Reward Threshold: {spec.reward_threshold}\")\n",
        "\n",
        "query_environment(\"Pendulum-v0\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Action Space: Box(1,)\n",
            "Observation Space: Box(3,)\n",
            "Max Episode Steps: 200\n",
            "Nondeterministic: False\n",
            "Reward Range: (-inf, inf)\n",
            "Reward Threshold: None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5ZByZmMIjRK"
      },
      "source": [
        "class ReplayBuffer:\n",
        "\n",
        "  def __init__(self, mem_size, *input_dims, state_size, action_space_size):\n",
        "    self.mem_size = mem_size\n",
        "    self.mem_cntr = 0\n",
        "    self.state_size = state_size\n",
        "    self.action_space_size = action_space_size\n",
        "    self.state_memory = np.zeros((self.mem_size, self.state_size, 1), dtype=np.float32)\n",
        "    self.new_state_memory = np.zeros((self.mem_size, self.state_size, 1), dtype=np.float32)\n",
        "    self.action_memory = np.zeros((self.mem_size, self.action_space_size, ), dtype=np.float32)\n",
        "    self.reward_memory = np.zeros(self.mem_size, dtype=np.int32)\n",
        "    self.terminal_memory = np.zeros(self.mem_size, dtype=np.int32)\n",
        "    self.current_states = None\n",
        "\n",
        "  def store_transition(self, state, action, reward, state_, done):\n",
        "    index = self.mem_cntr % self.mem_size\n",
        "    self.state_memory[index] = state\n",
        "    self.new_state_memory[index] = state_\n",
        "    self.reward_memory[index] = reward\n",
        "    self.action_memory[index] = action\n",
        "    self.terminal_memory[index] = 1 - int(done)\n",
        "    self.mem_cntr += 1\n",
        "\n",
        "\n",
        "  def sample_buffer(self, batch_size):\n",
        "    max_mem = min(self.mem_cntr, self.mem_size)\n",
        "    batch = np.random.choice(max_mem, batch_size, replace=False)\n",
        "    states = self.state_memory[batch]\n",
        "    self.current_states = states\n",
        "    states_ = self.new_state_memory[batch]\n",
        "    rewards = self.reward_memory[batch]\n",
        "    actions = self.action_memory[batch]\n",
        "    terminal = self.terminal_memory[batch]\n",
        "    #actions = np.expand_dims(actions, axis=1)\n",
        "    return states, actions, rewards, states_, terminal\n",
        "\n",
        "  def sample_buffer_with_distance(self, batch_size, distance):\n",
        "    max_mem = min(self.mem_cntr, self.mem_size)\n",
        "    batch = np.random.choice(max_mem, batch_size, replace=False)\n",
        "    actions = []\n",
        "    for index, value in enumerate(batch):\n",
        "      should_pass = False\n",
        "      avg_action = [0.0 for _ in range(self.action_space_size)]\n",
        "      if abs(max_mem - 1 - value) <= distance:\n",
        "        np.delete(batch, index)\n",
        "        pass\n",
        "      for i in range(distance):\n",
        "        if self.terminal_memory[value + i] is True:\n",
        "          np.delete(batch, index)\n",
        "          should_pass = True\n",
        "          break\n",
        "      if should_pass is True:\n",
        "        pass\n",
        "      for i in range(distance):\n",
        "        avg_action = avg_action + self.action_memory[value + i]\n",
        "      avg_action = avg_action / distance\n",
        "      actions.append(avg_action)\n",
        "    states = self.state_memory[batch]\n",
        "    self.current_states = states\n",
        "    states_ = self.state_memory[batch + distance - 1]\n",
        "    rewards = self.reward_memory[batch]\n",
        "    terminal = self.terminal_memory[batch]\n",
        "    return states, actions, rewards, states_, terminal\n",
        "\n",
        "  def sample_buffer_with_distance_for_recurrent(self, batch_size, distance):\n",
        "    max_mem = min(self.mem_cntr, self.mem_size)\n",
        "    batch = np.random.choice(max_mem - distance - 1, batch_size, replace=False)\n",
        "    actions = [[] for _ in range(len(batch))]\n",
        "    for index, value in enumerate(batch):\n",
        "      should_pass = False\n",
        "      if abs(max_mem - 1 - value) <= distance:\n",
        "        np.delete(batch, index)\n",
        "        pass\n",
        "      for i in range(distance):\n",
        "        if self.terminal_memory[value + i] is True:\n",
        "          np.delete(batch, index)\n",
        "          should_pass = True\n",
        "          break\n",
        "      if should_pass is True:\n",
        "        pass\n",
        "      for i in range(distance):\n",
        "        actions[index].append(self.action_memory[value + i])\n",
        "    states = self.state_memory[batch]\n",
        "    self.current_states = states\n",
        "    actions = np.array(actions)\n",
        "    actions = np.reshape(actions, (len(batch), distance*self.action_space_size,))\n",
        "    states_ = self.state_memory[batch + distance - 1]\n",
        "    rewards = self.reward_memory[batch]\n",
        "    terminal = self.terminal_memory[batch]\n",
        "\n",
        "    states_for_other_network = self.state_memory[batch]\n",
        "    next_states_for_other_network = self.state_memory[batch + 1]\n",
        "    actions_for_other_network = self.action_memory[batch]\n",
        "\n",
        "    return states, actions, rewards, states_, terminal, states_for_other_network, next_states_for_other_network, actions_for_other_network\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "381thIzvjZP7"
      },
      "source": [
        "class Agent:\n",
        "  def __init__(self, env, lr=0.01, state_size=4, action_size=2, optimal_state=[0.0, 0.0], gamma=0.01, epsilon=0.2, batch_size=5, num_epochs=1, epsilon_dec=1e-3, epsilon_end=0.01, mem_size=10000, layer_sizes=[64, 16], max_distance=5):\n",
        "    self.env = env\n",
        "    self.lr = lr\n",
        "    self.state_size = state_size\n",
        "    self.action_size = action_size\n",
        "    self.layer_sizes = layer_sizes\n",
        "    self.max_distance = max_distance\n",
        "    self.action_and_state_to_state_model = self.create_action_and_state_to_state_model(layer_sizes=[64, 64], obs_space_size=4)\n",
        "    self.state_and_state_to_action_model = self.create_state_and_state_to_action_model(self.layer_sizes, obs_space_size=self.state_size)\n",
        "    self.state_and_state_to_action_recurrent_model = self.create_state_and_state_to_action_recurrent_model(self.layer_sizes, obs_space_size=self.state_size, time_distance=self.max_distance)\n",
        "    #self.optimal_state = [0.0 for _ in range(self.state_size)] # cart position, angle\n",
        "    self.optimal_state = optimal_state\n",
        "    self.current_state = None\n",
        "    self.selected_action = None\n",
        "    self.prev_state = None\n",
        "    self.gamma = gamma\n",
        "    self.epsilon = epsilon\n",
        "    self.batch_size = batch_size\n",
        "    self.num_epochs = num_epochs\n",
        "    self.epsilon_dec = epsilon_dec\n",
        "    self.epsilon_end = epsilon_end\n",
        "    self.mem_size = mem_size\n",
        "    self.memory = ReplayBuffer(mem_size, (self.state_size, 1), state_size=self.state_size, action_space_size=self.action_size)\n",
        "    \n",
        "\n",
        "  def create_action_and_state_to_state_model(self, layer_sizes=[32, 16], obs_space_size=4):\n",
        "    action_input = Input(shape=(self.action_size, ), name='actionInput')\n",
        "    state_input = Input(shape=(obs_space_size, ), name='stateInput')\n",
        "    x = tf.keras.layers.concatenate([action_input, state_input], axis=1)\n",
        "    hidden_1 = Dense(layer_sizes[0], activation='tanh')(x)\n",
        "    hidden_2 = Dense(layer_sizes[1], activation='tanh')(hidden_1)\n",
        "    out = Dense(obs_space_size, activation='linear', name='outputState')(hidden_2)\n",
        "    model = tf.keras.Model(\n",
        "    inputs=[action_input, state_input],\n",
        "    outputs=out,\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(self.lr),\n",
        "    loss=tf.keras.losses.MeanSquaredError()\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "  def create_state_and_state_to_action_model(self, layer_sizes=[32, 16], obs_space_size=4):\n",
        "    current_state_input = Input(shape=(obs_space_size, ), name='currentStateInput')\n",
        "    target_state_input = Input(shape=(obs_space_size, ), name='targetStateInput')\n",
        "    x = tf.keras.layers.concatenate([current_state_input, target_state_input], axis=1)\n",
        "    hidden_1 = Dense(layer_sizes[0], activation='tanh')(x)\n",
        "    hidden_2 = Dense(layer_sizes[1], activation='tanh')(hidden_1)\n",
        "    action = Dense(self.action_size, activation='softmax', name='outputAction')(hidden_2)\n",
        "    model = tf.keras.Model(\n",
        "    inputs=[current_state_input, target_state_input],\n",
        "    outputs=action,\n",
        "    )\n",
        "    model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(self.lr),\n",
        "    loss=tf.keras.losses.CategoricalCrossentropy()\n",
        "    )\n",
        "    return model  \n",
        "    \n",
        "\n",
        "  def create_state_and_state_to_action_recurrent_model(self, layer_sizes=[32, 16], obs_space_size=4, time_distance=5):\n",
        "    current_state_input = Input(shape=(obs_space_size, ), name='currentStateInput')\n",
        "    target_state_input = Input(shape=(obs_space_size, ), name='targetStateInput')\n",
        "    x = tf.keras.layers.concatenate([current_state_input, target_state_input], axis=1)\n",
        "    hidden_1 = Dense(layer_sizes[0], activation='tanh')(x)\n",
        "    hidden_2 = Dense(layer_sizes[1], activation='tanh')(hidden_1)\n",
        "    hidden_3 = Dense(32, activation='tanh')(hidden_2)\n",
        "    #hidden_4 = BatchNormalization()(hidden_3)\n",
        "    action = Dense(time_distance*self.action_size, activation='sigmoid', name='outputAction')(hidden_3)\n",
        "    model = tf.keras.Model(\n",
        "    inputs=[current_state_input, target_state_input],\n",
        "    outputs=action,\n",
        "    )\n",
        "    model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(self.lr),\n",
        "    loss=tf.keras.losses.MeanAbsoluteError()\n",
        "    )\n",
        "    return model  \n",
        "\n",
        "\n",
        "  def update_network_batched(self, max_distance, randomize):\n",
        "    if self.memory.mem_cntr < self.batch_size:\n",
        "      return\n",
        "    distance = max_distance\n",
        "    if randomize is True:\n",
        "      distance = np.random.choice(range(2, max_distance+1))\n",
        "    states, actions, rewards, states_, done = self.memory.sample_buffer_with_distance(batch_size=self.batch_size, distance=distance)\n",
        "    self.state_and_state_to_action_model.fit(\n",
        "    {\"currentStateInput\": states, \"targetStateInput\": states_},\n",
        "    {\"outputAction\": np.expand_dims(np.array(actions), -1)},\n",
        "    epochs=self.num_epochs,\n",
        "    batch_size=self.batch_size,\n",
        "    verbose=0\n",
        "    )\n",
        "\n",
        "  def update_network_batched_recurrent(self, max_distance, randomize):\n",
        "    if self.memory.mem_cntr < self.batch_size:\n",
        "      return\n",
        "    distance = max_distance\n",
        "    if randomize is True:\n",
        "      distance = np.random.choice(range(2, max_distance+1))\n",
        "    states, actions, rewards, states_, done, states_for_other_network, next_states_for_other_network, actions_for_other_network = self.memory.sample_buffer_with_distance_for_recurrent(batch_size=self.batch_size, distance=distance)\n",
        "\n",
        "    self.action_and_state_to_state_model.fit(\n",
        "    {\"actionInput\": np.asarray(actions_for_other_network), \"stateInput\": np.asarray(states_for_other_network)},\n",
        "    {\"outputState\": np.asarray(next_states_for_other_network)},\n",
        "    epochs=self.num_epochs,\n",
        "    batch_size=self.batch_size,\n",
        "    verbose=0\n",
        "    )\n",
        "\n",
        "\n",
        "    self.state_and_state_to_action_recurrent_model.fit(\n",
        "    {\"currentStateInput\": states, \"targetStateInput\": states_},\n",
        "    {\"outputAction\": np.expand_dims(np.array(actions), -1)},\n",
        "    epochs=self.num_epochs,\n",
        "    batch_size=self.batch_size,\n",
        "    verbose=0\n",
        "    )\n",
        "\n",
        "\n",
        "  def store_transition(self, state, action, reward, new_state, done):\n",
        "    self.memory.store_transition(np.expand_dims(state, axis=1), action, reward, np.expand_dims(new_state, axis=1) , done)\n",
        "\n",
        "\n",
        "  def choose_action_from_the_states(self, currentState, targetState):\n",
        "    if np.random.random() < self.epsilon:\n",
        "      action = np.random.choice(self.action_size)\n",
        "    else:\n",
        "      state = np.array([currentState])\n",
        "      predicted_action = self.state_and_state_to_action_model([{\"targetStateInput\": np.expand_dims(np.asarray(self.optimal_state, dtype=np.float32), axis=0), \"currentStateInput\": state}])\n",
        "      self.selected_action = predicted_action\n",
        "      action = np.argmax(predicted_action[0])\n",
        "    return action\n",
        "\n",
        "  def choose_actions_from_the_states(self, currentState, targetState, distance):\n",
        "    if np.random.random() < self.epsilon:\n",
        "      actions = []\n",
        "      for _ in range(distance):\n",
        "        action = np.random.choice(self.action_size)\n",
        "        actions.append(action)\n",
        "      return actions\n",
        "    else:\n",
        "      state = np.array([currentState])\n",
        "      predicted_actions = self.state_and_state_to_action_recurrent_model([{\"targetStateInput\": np.expand_dims(np.asarray(self.optimal_state, dtype=np.float32), axis=0), \"currentStateInput\": state}])\n",
        "      predicted_actions = np.reshape(np.array(predicted_actions), (distance, self.action_size))\n",
        "      predicted_actions_final = map(lambda actions: np.argmax(actions), predicted_actions)\n",
        "      return list(predicted_actions_final)\n",
        "\n",
        "  def to_one_hot(self, action_index):\n",
        "    vec = np.zeros((self.action_size,))\n",
        "    vec[action_index] = 1\n",
        "    return vec  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "je8UMXuDQ2BX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "d30dedc7-bad2-4ab3-99f8-776ee5df9fac"
      },
      "source": [
        "import math\n",
        "#env = wrap_env(gym.make(\"CartPole-v1\"))\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "#env = gym.make(\"MountainCar-v0\")\n",
        "#To tune: num_games, batch_size, activation functions, distance, epsilon\n",
        "\n",
        "num_games = 500\n",
        "max_distance = 3\n",
        "batch_size = 16\n",
        "num_epochs = 1\n",
        "\n",
        "#CartPole\n",
        "state_size = 4\n",
        "action_size = 2\n",
        "optimal_state = [0.0, 0.0, 0.0, 0.0]\n",
        "\n",
        "\n",
        "#MountainCar\n",
        "# state_size = 2\n",
        "# action_size = 3\n",
        "# optimal_state = [-1.2, -0.07]\n",
        "\n",
        "agent = Agent(env, batch_size=batch_size, state_size = state_size, action_size=action_size, optimal_state=optimal_state,\n",
        "              num_epochs=num_epochs, lr=0.0001, epsilon=0.20, epsilon_dec=0.90, epsilon_end=0.10, layer_sizes=[128, 128], max_distance=max_distance)\n",
        "rewards = [0.0 for _ in range(num_games)]\n",
        "steps = [0 for _ in range(num_games)]\n",
        "# state -> array of 4: 0 cart position, 1 cart velocity, 2 angle, 3 velocity at tip\n",
        "for index in range(num_games):\n",
        "  current_state = env.reset()\n",
        "  # if index < 50:\n",
        "  #   env.state = np.array(optimal_state)\n",
        "  #env.state = np.array([0.0, 0.0, 0.0, 0.0])\n",
        "  should_reset = False\n",
        "  while True:\n",
        "      #env.render()\n",
        "      agent.prev_state = current_state\n",
        "      selected_actions_array = agent.choose_actions_from_the_states(current_state, agent.optimal_state, max_distance)\n",
        "      for selected_action_index in selected_actions_array:\n",
        "        agent.selected_action = agent.to_one_hot(selected_action_index)\n",
        "        new_state, reward, done, info = env.step(selected_action_index)\n",
        "        rewards[index] = rewards[index] + reward\n",
        "        agent.current_state = new_state\n",
        "        agent.store_transition( current_state, agent.selected_action, reward, new_state, done)\n",
        "        current_state = new_state\n",
        "        steps[index] += 1\n",
        "        if done:\n",
        "          should_reset = True \n",
        "          break;\n",
        "        if agent.epsilon >= agent.epsilon_end:\n",
        "          agent.epsilon = agent.epsilon * agent.epsilon_dec\n",
        "      if should_reset:\n",
        "        break\n",
        "  \n",
        "  agent.update_network_batched_recurrent(max_distance, randomize=False)\n",
        "  \n",
        "\n",
        "#print(sum(steps) / num_games)\n",
        "#print(sum(rewards[math.ceil(len(rewards) / 2):]) / num_games)  \n",
        "print(np.median(rewards[math.ceil(len(rewards) / 2):]))   \n",
        "print(rewards[math.ceil(len(rewards) / 2):])      \n",
        "env.close()\n",
        "#show_video()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "48.5\n",
            "[27.0, 29.0, 20.0, 79.0, 40.0, 55.0, 34.0, 12.0, 47.0, 57.0, 48.0, 46.0, 39.0, 96.0, 42.0, 63.0, 46.0, 54.0, 43.0, 41.0, 84.0, 63.0, 65.0, 30.0, 45.0, 65.0, 87.0, 34.0, 42.0, 47.0, 49.0, 64.0, 37.0, 42.0, 32.0, 65.0, 78.0, 45.0, 64.0, 59.0, 50.0, 31.0, 71.0, 47.0, 57.0, 52.0, 59.0, 41.0, 40.0, 110.0, 35.0, 40.0, 41.0, 22.0, 40.0, 23.0, 50.0, 113.0, 52.0, 66.0, 36.0, 61.0, 62.0, 48.0, 78.0, 29.0, 53.0, 58.0, 25.0, 35.0, 76.0, 47.0, 42.0, 28.0, 73.0, 71.0, 54.0, 56.0, 71.0, 102.0, 34.0, 80.0, 41.0, 43.0, 47.0, 30.0, 87.0, 40.0, 57.0, 83.0, 40.0, 70.0, 34.0, 60.0, 48.0, 49.0, 38.0, 24.0, 62.0, 28.0, 60.0, 34.0, 29.0, 28.0, 34.0, 18.0, 55.0, 60.0, 50.0, 31.0, 76.0, 38.0, 58.0, 55.0, 46.0, 45.0, 46.0, 51.0, 82.0, 40.0, 98.0, 38.0, 52.0, 72.0, 40.0, 77.0, 50.0, 27.0, 128.0, 46.0, 54.0, 55.0, 66.0, 40.0, 25.0, 66.0, 57.0, 70.0, 47.0, 60.0, 65.0, 35.0, 69.0, 127.0, 110.0, 38.0, 107.0, 51.0, 60.0, 40.0, 28.0, 55.0, 40.0, 55.0, 21.0, 30.0, 55.0, 86.0, 77.0, 102.0, 23.0, 47.0, 40.0, 61.0, 68.0, 50.0, 80.0, 57.0, 34.0, 62.0, 88.0, 33.0, 157.0, 53.0, 35.0, 67.0, 60.0, 30.0, 73.0, 46.0, 24.0, 91.0, 93.0, 42.0, 35.0, 49.0, 49.0, 70.0, 57.0, 106.0, 93.0, 15.0, 42.0, 47.0, 79.0, 34.0, 44.0, 21.0, 49.0, 34.0, 102.0, 56.0, 72.0, 80.0, 61.0, 35.0, 59.0, 42.0, 42.0, 48.0, 27.0, 63.0, 41.0, 27.0, 27.0, 69.0, 42.0, 17.0, 34.0, 53.0, 16.0, 41.0, 46.0, 29.0, 21.0, 36.0, 20.0, 27.0, 32.0, 40.0, 40.0, 63.0, 84.0, 81.0, 66.0, 78.0, 48.0, 104.0, 46.0, 46.0, 49.0, 71.0, 46.0, 54.0, 49.0, 28.0, 56.0, 53.0, 36.0, 47.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXkIEFazR9CM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "c688c56a-2c25-42a5-cadc-014590e6b544"
      },
      "source": [
        "from scipy.spatial import distance\n",
        "#env = wrap_env(gym.make(\"CartPole-v1\"))\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "#env = gym.make(\"MountainCar-v0\")\n",
        "#To tune: num_games, batch_size, activation functions, distance, epsilon\n",
        "\n",
        "num_games = 100\n",
        "max_distance = 4\n",
        "batch_size = 16\n",
        "num_epochs = 1\n",
        "\n",
        "#CartPole\n",
        "state_size = 4\n",
        "action_size = 2\n",
        "optimal_state = [0.0, 0.0, 0.0, 0.0]\n",
        "\n",
        "#MountainCar\n",
        "# state_size = 2\n",
        "# action_size = 3\n",
        "# optimal_state = [-1.2, -0.07]\n",
        "\n",
        "def step(agent, current_state, should_reset, current_index, rewards, steps, step_depth):\n",
        "  if should_reset:\n",
        "    return True\n",
        "  agent.prev_state = current_state\n",
        "  selected_actions_array = agent.choose_actions_from_the_states(current_state, agent.optimal_state, max_distance)\n",
        "  for selected_action_index in selected_actions_array:\n",
        "    agent.selected_action = agent.to_one_hot(selected_action_index)\n",
        "    predicted_next_state = agent.action_and_state_to_state_model([{\"actionInput\": np.array([agent.selected_action]), \"stateInput\": np.array([agent.prev_state])}])\n",
        "    new_state, reward, done, info = env.step(selected_action_index)\n",
        "    if done:\n",
        "      should_reset = True \n",
        "      return True;\n",
        "    # difference_between_states = scipy.spatial.distance.euclidean(np.expand_dims(new_state, axis=1), np.transpose(np.array(predicted_next_state)))\n",
        "    # percentage_error = difference_between_states / scipy.linalg.norm(new_state)\n",
        "    cosine_similarity = 1 - scipy.spatial.distance.cosine(np.expand_dims(new_state, axis=1), np.transpose(np.array(predicted_next_state)))\n",
        "    if( (cosine_similarity < 0.85) and step_depth < 2):\n",
        "      step_depth += 1\n",
        "      should_break = step(agent, new_state, should_reset, current_index, rewards, steps, step_depth)\n",
        "      rewards[index] = rewards[index] + reward\n",
        "      agent.current_state = new_state\n",
        "      agent.store_transition( current_state, agent.selected_action, reward, new_state, done)\n",
        "      current_state = new_state\n",
        "      steps[index] += 1\n",
        "      if agent.epsilon >= agent.epsilon_end:\n",
        "        agent.epsilon = agent.epsilon * agent.epsilon_dec\n",
        "      if should_break:\n",
        "        return True\n",
        "    rewards[index] = rewards[index] + reward\n",
        "    agent.current_state = new_state\n",
        "    agent.store_transition( current_state, agent.selected_action, reward, new_state, done)\n",
        "    current_state = new_state\n",
        "    steps[index] += 1\n",
        "    if agent.epsilon >= agent.epsilon_end:\n",
        "      agent.epsilon = agent.epsilon * agent.epsilon_dec\n",
        "  if should_reset:\n",
        "    return True\n",
        "  else:\n",
        "    return False\n",
        "\n",
        "\n",
        "agent = Agent(env, batch_size=batch_size, state_size = state_size, action_size=action_size, optimal_state=optimal_state,\n",
        "              num_epochs=num_epochs, lr=0.00001, epsilon=0.05, epsilon_dec=0.90, epsilon_end=0.10, layer_sizes=[64, 64], max_distance=max_distance)\n",
        "rewards = [0.0 for _ in range(num_games)]\n",
        "steps = [0 for _ in range(num_games)]\n",
        "# state -> array of 4: 0 cart position, 1 cart velocity, 2 angle, 3 velocity at tip\n",
        "for index in range(num_games):\n",
        "  current_state = env.reset()\n",
        "  should_reset = False\n",
        "  while True:\n",
        "      #env.render()\n",
        "      step_depth = 0\n",
        "      should_break = step(agent, current_state, should_reset, index, rewards, steps, step_depth)\n",
        "      if should_break or should_reset:\n",
        "        break\n",
        "  \n",
        "  agent.update_network_batched_recurrent(max_distance, randomize=False)\n",
        "  \n",
        "\n",
        "#print(sum(steps) / num_games)\n",
        "print(sum(rewards) / num_games)  \n",
        "print(np.median(rewards))   \n",
        "print(rewards)      \n",
        "env.close()\n",
        "#show_video()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "44.59\n",
            "39.0\n",
            "[97.0, 49.0, 112.0, 21.0, 27.0, 36.0, 19.0, 19.0, 49.0, 23.0, 22.0, 39.0, 50.0, 26.0, 81.0, 20.0, 22.0, 24.0, 31.0, 23.0, 14.0, 43.0, 22.0, 49.0, 39.0, 58.0, 60.0, 24.0, 26.0, 128.0, 84.0, 50.0, 40.0, 20.0, 23.0, 42.0, 58.0, 74.0, 34.0, 91.0, 52.0, 50.0, 49.0, 32.0, 51.0, 38.0, 18.0, 71.0, 22.0, 49.0, 45.0, 41.0, 77.0, 22.0, 26.0, 38.0, 30.0, 77.0, 61.0, 22.0, 71.0, 62.0, 47.0, 26.0, 39.0, 34.0, 62.0, 21.0, 46.0, 26.0, 99.0, 39.0, 58.0, 23.0, 47.0, 40.0, 22.0, 26.0, 144.0, 40.0, 117.0, 21.0, 20.0, 27.0, 23.0, 55.0, 23.0, 113.0, 22.0, 40.0, 12.0, 22.0, 94.0, 46.0, 22.0, 14.0, 75.0, 37.0, 24.0, 40.0]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}